{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\ndevice = torch.device(\"cuda\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-02T15:46:54.780004Z","iopub.execute_input":"2021-12-02T15:46:54.780458Z","iopub.status.idle":"2021-12-02T15:46:54.790700Z","shell.execute_reply.started":"2021-12-02T15:46:54.780407Z","shell.execute_reply":"2021-12-02T15:46:54.789705Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:46:54.793373Z","iopub.execute_input":"2021-12-02T15:46:54.801009Z","iopub.status.idle":"2021-12-02T15:46:54.810282Z","shell.execute_reply.started":"2021-12-02T15:46:54.800950Z","shell.execute_reply":"2021-12-02T15:46:54.809305Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:46:54.812097Z","iopub.execute_input":"2021-12-02T15:46:54.812649Z","iopub.status.idle":"2021-12-02T15:47:03.673749Z","shell.execute_reply.started":"2021-12-02T15:46:54.812601Z","shell.execute_reply":"2021-12-02T15:47:03.672734Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:03.679341Z","iopub.execute_input":"2021-12-02T15:47:03.679636Z","iopub.status.idle":"2021-12-02T15:47:13.107848Z","shell.execute_reply.started":"2021-12-02T15:47:03.679596Z","shell.execute_reply":"2021-12-02T15:47:13.106570Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"351cc1ebc0d966d49152a4c1937915dd4e7b4ef5\"\n\nwandb.login(key=\"351cc1ebc0d966d49152a4c1937915dd4e7b4ef5\")\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:13.112151Z","iopub.execute_input":"2021-12-02T15:47:13.112951Z","iopub.status.idle":"2021-12-02T15:47:13.128251Z","shell.execute_reply.started":"2021-12-02T15:47:13.112914Z","shell.execute_reply":"2021-12-02T15:47:13.126623Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"wandb.init(project = \"Sentiment Analysis\")","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:13.130047Z","iopub.execute_input":"2021-12-02T15:47:13.130466Z","iopub.status.idle":"2021-12-02T15:47:26.236411Z","shell.execute_reply.started":"2021-12-02T15:47:13.130421Z","shell.execute_reply":"2021-12-02T15:47:26.234833Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"\n\npath_dataset = \"/kaggle/input/dataset-12-02/Dataset_02_12.xlsx\"\ndataframe = pd.read_excel(path_dataset, sheet_name = 'Dataset')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:26.238828Z","iopub.execute_input":"2021-12-02T15:47:26.239145Z","iopub.status.idle":"2021-12-02T15:47:27.978878Z","shell.execute_reply.started":"2021-12-02T15:47:26.239097Z","shell.execute_reply":"2021-12-02T15:47:27.977823Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:27.980647Z","iopub.execute_input":"2021-12-02T15:47:27.980973Z","iopub.status.idle":"2021-12-02T15:47:27.992397Z","shell.execute_reply.started":"2021-12-02T15:47:27.980929Z","shell.execute_reply":"2021-12-02T15:47:27.991304Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# **Split stratify Dataset**","metadata":{}},{"cell_type":"code","source":"\n\ntrain_text, val_text, train_labels, val_labels = train_test_split(dataframe['Review'], dataframe['Label'], \n                                                                    random_state=2021, \n                                                                    test_size=0.1, \n                                                                    stratify=dataframe['Label'])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:27.994217Z","iopub.execute_input":"2021-12-02T15:47:27.994823Z","iopub.status.idle":"2021-12-02T15:47:28.017773Z","shell.execute_reply.started":"2021-12-02T15:47:27.994741Z","shell.execute_reply":"2021-12-02T15:47:28.016813Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:28.019591Z","iopub.execute_input":"2021-12-02T15:47:28.019943Z","iopub.status.idle":"2021-12-02T15:47:37.226360Z","shell.execute_reply.started":"2021-12-02T15:47:28.019897Z","shell.execute_reply":"2021-12-02T15:47:37.225177Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:37.228907Z","iopub.execute_input":"2021-12-02T15:47:37.229270Z","iopub.status.idle":"2021-12-02T15:47:37.236483Z","shell.execute_reply.started":"2021-12-02T15:47:37.229223Z","shell.execute_reply":"2021-12-02T15:47:37.235290Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:37.239031Z","iopub.execute_input":"2021-12-02T15:47:37.241907Z","iopub.status.idle":"2021-12-02T15:47:41.880815Z","shell.execute_reply.started":"2021-12-02T15:47:37.241848Z","shell.execute_reply":"2021-12-02T15:47:41.879708Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(features.pooler_output)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:41.882641Z","iopub.execute_input":"2021-12-02T15:47:41.883327Z","iopub.status.idle":"2021-12-02T15:47:41.889651Z","shell.execute_reply.started":"2021-12-02T15:47:41.883278Z","shell.execute_reply":"2021-12-02T15:47:41.888462Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhoBert_Classification(torch.nn.Module):\n    def __init__(self, num_class):\n        super(PhoBert_Classification, self).__init__()\n        self.backbone = AutoModel.from_pretrained(\"vinai/phobert-base\")\n        \n        self.dense_1 = torch.nn.Linear(in_features = 768, out_features = 128, bias=True)\n        self.dense_2 = torch.nn.Linear(in_features = 128, out_features = num_class, bias=True)\n        self.dropout1 = nn.Dropout(0.1)\n        self.relu =  nn.ReLU()\n        self.dropout2 = nn.Dropout(0.1)\n        #softmax activation function (Log softmax)\n        self.softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, sent_id,mask):\n\n        #get pooler_output of ['CLS'] token from bert output\n        cls_hs= self.backbone(sent_id, attention_mask=mask).pooler_output\n        x = self.dropout1(cls_hs)\n        x = self.dense_1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout2(x)\n\n        # output layer\n        x = self.dense_2(x)\n\n        x = self.softmax(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:41.896045Z","iopub.execute_input":"2021-12-02T15:47:41.896798Z","iopub.status.idle":"2021-12-02T15:47:41.907493Z","shell.execute_reply.started":"2021-12-02T15:47:41.896719Z","shell.execute_reply":"2021-12-02T15:47:41.906428Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = PhoBert_Classification(2)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:41.910109Z","iopub.execute_input":"2021-12-02T15:47:41.910789Z","iopub.status.idle":"2021-12-02T15:47:44.607826Z","shell.execute_reply.started":"2021-12-02T15:47:41.910708Z","shell.execute_reply":"2021-12-02T15:47:44.606719Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in train_text]\npd.Series(seq_len).hist(bins = 30)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:44.609557Z","iopub.execute_input":"2021-12-02T15:47:44.609895Z","iopub.status.idle":"2021-12-02T15:47:45.003956Z","shell.execute_reply.started":"2021-12-02T15:47:44.609853Z","shell.execute_reply":"2021-12-02T15:47:45.002950Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\nMAX_LENGTH = 200\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = MAX_LENGTH,\n    pad_to_max_length=True,\n    truncation=True\n)\n\n# # tokenize and encode sequences in the test set\n# tokens_test = tokenizer.batch_encode_plus(\n#     test_text.tolist(),\n#     max_length = MAX_LENGTH,\n#     pad_to_max_length=True,\n#     truncation=True\n# )","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:45.005555Z","iopub.execute_input":"2021-12-02T15:47:45.006093Z","iopub.status.idle":"2021-12-02T15:47:48.035974Z","shell.execute_reply.started":"2021-12-02T15:47:45.006048Z","shell.execute_reply":"2021-12-02T15:47:48.035069Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\n# test_seq = torch.tensor(tokens_test['input_ids'])\n# test_mask = torch.tensor(tokens_test['attention_mask'])\n# test_y = torch.tensor(test_labels.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.037315Z","iopub.execute_input":"2021-12-02T15:47:48.039833Z","iopub.status.idle":"2021-12-02T15:47:48.570213Z","shell.execute_reply.started":"2021-12-02T15:47:48.039791Z","shell.execute_reply":"2021-12-02T15:47:48.565469Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 128\n\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\ntrain_sampler = RandomSampler(train_data)\n\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\nval_sampler = SequentialSampler(val_data)\n\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.576179Z","iopub.execute_input":"2021-12-02T15:47:48.578631Z","iopub.status.idle":"2021-12-02T15:47:48.589386Z","shell.execute_reply.started":"2021-12-02T15:47:48.578587Z","shell.execute_reply":"2021-12-02T15:47:48.588266Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# test_tensordata = TensorDataset(test_seq, test_mask, test_y)\n# test_sampler =  SequentialSampler(test_tensordata)\n# test_dataloader = DataLoader(test_tensordata, sampler = test_sampler, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.594739Z","iopub.execute_input":"2021-12-02T15:47:48.597635Z","iopub.status.idle":"2021-12-02T15:47:48.604084Z","shell.execute_reply.started":"2021-12-02T15:47:48.597570Z","shell.execute_reply":"2021-12-02T15:47:48.602946Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# **Build model with backbone and pretrained Bert base uncased**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.607321Z","iopub.execute_input":"2021-12-02T15:47:48.608324Z","iopub.status.idle":"2021-12-02T15:47:48.754121Z","shell.execute_reply.started":"2021-12-02T15:47:48.608275Z","shell.execute_reply":"2021-12-02T15:47:48.752786Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\noptimizer = AdamW(model.parameters(), lr = 1e-5) ","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.756118Z","iopub.execute_input":"2021-12-02T15:47:48.756712Z","iopub.status.idle":"2021-12-02T15:47:48.770374Z","shell.execute_reply.started":"2021-12-02T15:47:48.756667Z","shell.execute_reply":"2021-12-02T15:47:48.768946Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n\nprint(\"Class Weights:\",class_weights)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.772176Z","iopub.execute_input":"2021-12-02T15:47:48.772831Z","iopub.status.idle":"2021-12-02T15:47:48.792156Z","shell.execute_reply.started":"2021-12-02T15:47:48.772739Z","shell.execute_reply":"2021-12-02T15:47:48.790709Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 120","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.793990Z","iopub.execute_input":"2021-12-02T15:47:48.794650Z","iopub.status.idle":"2021-12-02T15:47:48.803132Z","shell.execute_reply.started":"2021-12-02T15:47:48.794605Z","shell.execute_reply":"2021-12-02T15:47:48.801561Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def train(model):\n  model.train()\n  total_loss, total_accuracy = 0, 0\n  total_preds=[]\n  for step,batch in enumerate(train_dataloader):\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n    model.zero_grad()        \n    preds = model(sent_id, mask)\n    loss = cross_entropy(preds, labels)\n    total_loss = total_loss + loss.item()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    preds=preds.detach().cpu().numpy()\n    total_preds.append(preds)\n\n  avg_loss = total_loss / len(train_dataloader)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.805603Z","iopub.execute_input":"2021-12-02T15:47:48.806309Z","iopub.status.idle":"2021-12-02T15:47:48.823575Z","shell.execute_reply.started":"2021-12-02T15:47:48.806264Z","shell.execute_reply":"2021-12-02T15:47:48.822342Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.826113Z","iopub.execute_input":"2021-12-02T15:47:48.827076Z","iopub.status.idle":"2021-12-02T15:47:48.836180Z","shell.execute_reply.started":"2021-12-02T15:47:48.827016Z","shell.execute_reply":"2021-12-02T15:47:48.834847Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, t_dataset_loader):\n  \n    print(\"\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_groundtruth = []\n\n    # iterate over batches\n    for step,batch in enumerate(t_dataset_loader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n            \n            out_labels = labels.detach().cpu().numpy()\n            total_groundtruth.append(out_labels)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0,)\n    total_preds = np.argmax(total_preds, axis=1)\n    total_preds = np.array(total_preds, dtype = np.int16)\n    total_groundtruth = np.concatenate(total_groundtruth, axis = 0)\n    total_groundtruth = np.array(total_groundtruth, dtype = np.int16)\n\n    #F1 score\n    focus_f1 = f1_score(total_groundtruth, total_preds)\n    print(\"Accuracy: \", accuracy_score(total_groundtruth, total_preds))\n    print(\"F1 score: \", focus_f1)\n    print('Recall:', recall_score(total_groundtruth, total_preds))\n    print('Precision:', precision_score(total_groundtruth, total_preds))\n    print('\\n clasification report:\\n', classification_report(total_groundtruth,total_preds))\n    print('\\n confussion matrix:\\n',confusion_matrix(total_groundtruth, total_preds))\n    \n\n\n    return avg_loss, total_preds, focus_f1","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.838441Z","iopub.execute_input":"2021-12-02T15:47:48.839185Z","iopub.status.idle":"2021-12-02T15:47:48.864147Z","shell.execute_reply.started":"2021-12-02T15:47:48.839141Z","shell.execute_reply":"2021-12-02T15:47:48.862457Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# **Training and Validation**","metadata":{}},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\nbest_valid_f1 = 0\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n    print(\"Start\")\n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #Freeze in 30 epoch first\n    if epoch <= 10:\n        for param in model.backbone.parameters():\n            param.requires_grad = False\n    else:\n        for param in model.bert.parameters():\n            param.requires_grad = True\n    \n    #train model\n    train_loss, _ = train(model)\n    wandb.log({\"Loss train\": train_loss})\n    #evaluate model\n    valid_loss, _, f1_value = evaluate(model, val_dataloader)\n    wandb.log({\"Loss val\": valid_loss})\n    wandb.log({\"F1 score\": f1_value})\n    #save the best model\n    if f1_value > best_valid_f1:\n        best_valid_f1 = f1_value\n        torch.save(model.state_dict(), '/kaggle/working/Best_weights_f1.pt')\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n    torch.save(model.state_dict(), '/kaggle/working/Lass_weights_f1.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"execution":{"iopub.status.busy":"2021-12-02T15:47:48.866380Z","iopub.execute_input":"2021-12-02T15:47:48.867138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Save and reload Pretrained**","metadata":{}},{"cell_type":"code","source":"# best_model = BERT_sentiment_analysis(bert)\n# best_model.load_state_dict(torch.load(\"../input/weight-model/Best_weights_f1.pt\",map_location=device))\n# best_model = best_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Eval in test dataset**","metadata":{}},{"cell_type":"code","source":"# _ = evaluate(best_model, test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def inference(model, string_input):\n#     model.eval()\n    \n#     tokens_inference = tokenizer.batch_encode_plus(\n#         [string_input],\n#         max_length = MAX_LENGTH,\n#         pad_to_max_length=True,\n#         truncation=True\n#         )\n#     inference_seq = torch.tensor(tokens_inference['input_ids'])\n#     inference_mask = torch.tensor(tokens_inference['attention_mask'])\n#     sent_id = inference_seq.to(device)\n#     mask = inference_mask.to(device)\n#     preds = model(sent_id, mask)\n#     preds = preds.detach().cpu().numpy()\n#     class_predict = np.argmax(preds, axis = 1)\n#     print(\"Class predict: \",class_predict[0])\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Input and inference a sentence**","metadata":{}},{"cell_type":"code","source":"# string_input = input(\"Enter your string: \")\n# inference(best_model, string_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}